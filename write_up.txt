Jeremy Bernstein & Travis Price
CS481 Operatting Systems
Spring 2013

Lab4 Write Up: OS Measurement and Analysis

  Part 1

1) Function Calls: `./os_bench 1` and `./os_bench 2`

What we would like to measure here by calling system and non-system function calls is: how long does it take for a simple function call to execute. In the case of a non-system call, the process would be that of pushing the function onto the stack, executing the function, and pushing off the stack. In the case of a system function, the OS is trapped, the function executed, and control returned to the original program. For both of these, we want to simplify the functions called to reduce the overhead. Simple functions are the bane of a compiler, so to make sure the test code we write isn't tampered with we turn off compiler optimization in gcc with the -O0 flag (0 Optimization level).

The Non-System function call simply increments a value and returns. In our experiment we start a timer, call the function, and clock the timer when the process returns to calling function. The experiment is repeated a large number (currently 10 million) times, and the results are averaged.

The results are: Average time per test: 29.025314 ns/test

The System function call simply times the  call `syscall(SYS_gettid)` a large number of times (currently 10 million), and averages the results. The system call we are invoiking is `SYS_gettid` which returns the pid_t id of the current thread. This is generally considered of the lightweight system calls.

The results are: Average time per test: 64.311859 ns/test

As we might expect, local function calls operate faster. When a system call is executed, the OS is trapped, a lightweight context switch occurs, the command is executed by the OS, and the function returns. Naturally this is more complicated than just pushing a function onto the local processes' stack. However it is quite remarkable how optimized the system function call is. 


2) Task Creation `./os_bench 3` and `./os_bench 4`

What we would like to measure here is the cost of creating a process and a thread respectivley. Recall that a process is created via fork() and has its own registers, address space, page table, etc. A thread on the other had has its own registers, but shares the address space and page table with it's parent. One might expect (correctly) that process creation is more complicated, and thus slower.

The process creation experiment is run as follows: a timer is started, `fork()` is called, the parent thread continues into it's own block and stops the timer. The time captured reflects how long it takes for the OS to set up a new process via the `fork()` and then return control back to the parent process. The child process simply exits. This is repeated a large number of times (currently 100,000), and the results are averaged.

The results are: Average time per test: 57520.558594 ns/test

The thread creation experiment is run as follows: a timer is started, `pthread_create` is called to create a new process, the timer is stopped. This effectively captures how long it takes for a process to create a new thread. This is repeated a large number of times (currently 100,000), and the results are averaged.

The results are: Average time per test: 7404.165039 ns/test

As expected, thread creation is significantly faster than process creation. We initially implemented the test in a different manner, where the timer was stopped by the newly created thread/process. However we decided that this would not necissarily measure the time to create a function/thread, as we have no garuntee when that new thread/process will run.


3) Context Switching `./os_bench 5` and `./os_bench 6`

What we would like to measure here is the cost of switching between two processes/threads. Recall the steps of a context switch: first the OS is either trapped into running, or it intervenes and halts a process. Next it stores the current values of the running process's registers into the Process Control Block so it can be restored later. The next process is loaded up and the register values from the PCB are inserted into the CPU. The new process begins execution. We would like to measure the time of this process, meaning we want to trigger timers right before the context switch happens, and immediatly again once the new process starts running.

The Process Context Switch Benchmark is run as follows: pipes are created for the process to talk to the process it will switch to. The process is set to operate on one CPU using `sched_setaffinity` (child processes will inherit this). Next the parent process splits with `fork`. Begin loop: The child process blocks while waiting to read from a pipe to the parent. The parent starts it's timer and sends a message down to the child. The child, on recieving the message wakes up and stops the clock. The current time is then sent back to the parent so it can be logged. This process is repeated a large number (currently 10 million) of times and averaged.

The results are: Average time per test: 1110.617432 ns/test

The Thread Context Switch Benchmark is run as follows: The process (and thread once created) are set to run on the same processor. Next a thread is created with `pthread_create` and the child thread goes to sleep waiting for a signal from the parent. The parent runs, starting a timer, signals the child process, and goes to sleep. The child, upon recieveing the condition signal to awaken, stops the timer. This process is repeated a large number (currently 10 million) of times and averaged.

The results are: Average time per test: 968.851868 ns/test

Context switching between threads is slightly (insignificantly) faster than a process context switch. This might be caused by various forms of overhead revolving around a process context switch having it's own address space, and such as needing to flush the TLB.


4) Extra Credit

  Part 2

5) Multi-Threaded Program

6) Multi-Process Program

7) Measurements using 'time'

8) Problems or deficiences with 'time'

  Part 3

The Hypotheses

9) Expected Performance of 'read', 'fread', and 'mmap/memcpy'

Based on some reading of man pages, and wikipedia pages, we hypothesize the following: read will be the fastest function, it will be faster than fread because fread introduces extra overhead that eventually calls a standard read. mmap comes in last, via wikipedia: "... initally file contents are not entirely read from disk and do not use pysical RAM at all. The actual reads from disk are performed in a "lazy" manner, after a specific location is accessed." 

10) Increasing Strides for 'read'

We expect that read will increase in efficiency at first (reading ~1 byte at a time being quite inefficient), then level off (1000+ bytes) as the value read reaches a standard block size. 

11) Increasing Strides for 'fread'

`fread`'s performance should behave roughly like `read` only slightly slower.

12) Increasing Strides for 'mmap/memcpy'

`mmap/memcpy` will have to go to disk even if the physical memory has already been mapped, therefore it should stay fairly consistantly slow.

Experimental Results

Raw Data:


Size of read: 1 
  Total runtime: 1333123688  
  Number of reads: 100000000  
  time/read: 13.331237 
  byte/time: 0.075012  
  time/byte: 13.331237  

Size of read: 10 
  Total runtime: 137606057  
  Number of reads: 10000000  
  time/read: 13.760607 
  byte/time: 0.726712  
  time/byte: 1.376061  

Size of read: 100 
  Total runtime: 25758316  
  Number of reads: 1000000  
  time/read: 25.758316 
  byte/time: 3.882241  
  time/byte: 0.257583  

Size of read: 1000 
  Total runtime: 14287916  
  Number of reads: 100000  
  time/read: 142.879166 
  byte/time: 6.998921  
  time/byte: 0.142879  

Size of read: 10000 
  Total runtime: 11752101  
  Number of reads: 10000  
  time/read: 1175.210083 
  byte/time: 8.509117  
  time/byte: 0.117521  


Results for read()

Size of read: 1 
  Total runtime: 14179403909  
  Number of reads: 100000000  
  time/read: 141.794037 
  byte/time: 0.007052  
  time/byte: 141.794037  

Size of read: 10 
  Total runtime: 1505690836  
  Number of reads: 10000000  
  time/read: 150.569092 
  byte/time: 0.066415  
  time/byte: 15.056909  

Size of read: 100 
  Total runtime: 174259435  
  Number of reads: 1000000  
  time/read: 174.259445 
  byte/time: 0.573857  
  time/byte: 1.742594  

Size of read: 1000 
  Total runtime: 22910067  
  Number of reads: 100000  
  time/read: 229.100677 
  byte/time: 4.364893  
  time/byte: 0.229101  

Size of read: 10000 
  Total runtime: 9905886  
  Number of reads: 10000  
  time/read: 990.588623 
  byte/time: 10.095008  
  time/byte: 0.099059  


Results for mmap()/memcpy

Size of read: 1 
  Total runtime: 444940602  
  Number of reads: 100000000  
  time/read: 4.449406 
  byte/time: 0.224749  
  time/byte: 4.449406  

Size of read: 10 
  Total runtime: 46911686  
  Number of reads: 10000000  
  time/read: 4.691169 
  byte/time: 2.131665  
  time/byte: 0.469117  

Size of read: 100 
  Total runtime: 8194026  
  Number of reads: 1000000  
  time/read: 8.194026 
  byte/time: 12.204013  
  time/byte: 0.081940  

Size of read: 1000 
  Total runtime: 8837626  
  Number of reads: 100000  
  time/read: 88.376259 
  byte/time: 11.315256  
  time/byte: 0.088376  

Size of read: 10000 
  Total runtime: 6653943  
  Number of reads: 10000  
  time/read: 665.394287 
  byte/time: 15.028683  
  time/byte: 0.066539  


Results and Summary:

Conclusions:
